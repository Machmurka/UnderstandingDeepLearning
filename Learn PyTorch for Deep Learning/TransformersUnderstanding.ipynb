{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=U0s0f995w14&t\n",
    "# https://arxiv.org/pdf/1706.03762.pdf\n",
    "# https://peterbloem.nl/blog/transformers \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weâ€™ll represent the input, a sequence of t vectors of dimension k as a t by k matrix ð—.\n",
    "# Including a minibatch dimension b, gives us an input tensor of size (b,t,k).\n",
    "# assume we have some tensor x with size (b, t, k)\n",
    "x=torch.randn((32,3,2))\n",
    "raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "\n",
    "# print(f\"Raw weights shape {raw_weights.shape}\\n\")\n",
    "# print(x[:1,])\n",
    "# print(raw_weights[:1,])\n",
    "\n",
    "weights=torch.softmax(raw_weights,dim=2)\n",
    "\n",
    "y = torch.bmm(weights, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,embed_size, heads):\n",
    "        super(SelfAttention,self).__init__()\n",
    "    \n",
    "        self.embed_size=embed_size    \n",
    "        self.heads = heads\n",
    "        self.head_dim=embed_size//heads \n",
    "        # These compute the queries, keys and values for all\n",
    "        # heads\n",
    "        assert(self.head_dim * heads==embed_size), \"Embed size needs to be divided by heads \\n \" \n",
    "        self.keys  =  nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries =  nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.values  =  nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out       =  nn.Linear(self.head_dim*heads, embed_size)\n",
    "\n",
    "\t   \n",
    "\n",
    "\n",
    "    def forward(self, values,keys,query,mask):\n",
    "        \n",
    "        N= query.shape[0]\n",
    "\n",
    "        value_len,key_len,query_len=values.shape[1],keys[1],query.shape[1]\n",
    "\n",
    "        #split embedding into self.heads pices\n",
    "        values =values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys= keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query= query.reshape(N, query_len, self .heads, self.head_dim)\n",
    "\n",
    "        #to explore in future instead of torch.bmm \n",
    "        #we multiply matrix querry, keys to use in attention(q,k,v) (1) https://arxiv.org/pdf/1706.03762.pdf \n",
    "        # queries shape :(N,query_len,heads,heads_dim)\n",
    "        # keys shape : (N,key_len,heads, heads_dim)\n",
    "         \n",
    "        energy=torch.einsum(\"nqdh,nkhd->nhqk\",[query,keys])\n",
    "        # energy shape (N,heads,query_len,key_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy=energy.masked_fill(mask=0, value=float(\"-1e20\"))\n",
    "        \n",
    "\n",
    "        attention=torch.softmax(energy/(self.embed_size**(1/2)), dim=3) #dim=3 to softmax on\n",
    "\n",
    "        # attention shape (N,heads,query_len , key_len)\n",
    "        # velues shape (N,value_len,heads,head_dim)\n",
    "        out=torch.einsum(\"nhql,nlhd -> nqhd\",[attention,values]).reshape(\n",
    "            N,query_len,self.heads*self.head_dim\n",
    "        ) \n",
    "\n",
    "        # out shape (N,query_len,heads,head_dim) then flatten last two dimensitons\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, embed_size, heads,dropout,forward_expansion):\n",
    "    super(TransformerBlock,self).__init__()\n",
    "\n",
    "    self.attention = SelfAttention(embed_size, heads=heads)\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(embed_size)\n",
    "    self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    self.ff = nn.Sequential(\n",
    "      nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "      nn.ReLU(), \n",
    "      nn.Linear(forward_expansion * embed_size, embed_size))\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, value,key,query,mask):\n",
    "    attention = self.attention(value,key,query,mask)\n",
    "\n",
    "    x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "    fedforward = self.ff(x)\n",
    "\n",
    "    return self.dropout(self.norm2(fedforward + x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) \n",
    "        super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_emb = nn.Embedding(num_tokens, k)\n",
    "        self.pos_emb = nn.Embedding(seq_length, k)\n",
    "\n",
    "\t\t# The sequence of transformer blocks that does all the\n",
    "\t\t# heavy lifting\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(k=k, heads=heads))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "\t\t# Maps the final output sequence to class logits\n",
    "        self.toprobs = nn.Linear(k, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A (b, t) tensor of integer values representing\n",
    "                  words (in some predetermined vocabulary).\n",
    "        :return: A (b, c) tensor of log-probabilities over the\n",
    "                 classes (where c is the nr. of classes).\n",
    "        \"\"\"\n",
    "\t\t# generate token embeddings\n",
    "        tokens = self.token_emb(x)\n",
    "        b, t, k = tokens.size()\n",
    "\n",
    "        # generate position embeddings\n",
    "        positions = torch.arange(t)\n",
    "        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
    "\n",
    "        x = tokens + positions\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        # Average-pool over the t dimension and project to class\n",
    "        # probabilities\n",
    "        x = self.toprobs(x.mean(dim=1))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
