{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1d3faebbfd0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from typing import Tuple, Dict, List\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "# https://medium.com/@karuneshu21/resnet-paper-walkthrough-b7f3bdba55f0\n",
    "# https://arxiv.org/pdf/1512.03385.pdf\n",
    "# https://medium.com/@karuneshu21/how-to-resnet-in-pytorch-9acb01f36cf5\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderCustom(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        # print(self.paths)\n",
    "        self.classes, self.class_to_idx = self.find_classes(targ_dir)\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path) \n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        img = self.load_image(index)\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return img, class_idx # return data, label (X, y)\n",
    "        \n",
    "    def find_classes(self,directory:str)->Tuple[list[str],dict[str,int]]:\n",
    "        classes=sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "        class_to_idx={cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        \n",
    "        print(classes,class_to_idx)\n",
    "        return classes,class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataTest():\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.train_dir = \"C:\\\\Users\\\\Jakub Machura\\\\source\\\\repos\\\\UnderstandingDeepLearning\\\\data\\\\pizza_steak_sushi\\\\train\"\n",
    "        self.test_dir = \"C:\\\\Users\\\\Jakub Machura\\\\source\\\\repos\\\\UnderstandingDeepLearning\\\\data\\\\pizza_steak_sushi\\\\test\"\n",
    "        \"\"\"\n",
    "            temp placement for of transform\n",
    "        \"\"\"\n",
    "        self.train_transform=transforms.Compose([\n",
    "            transforms.Resize(size=(224,224)),\n",
    "            # transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "        self.test_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "        self.train_data =ImageFolderCustom(self.train_dir,transform=self.train_transform)\n",
    "        self.test_data=ImageFolderCustom(self.test_dir,transform=self.test_transforms)\n",
    "    \n",
    "        \"\"\"temp place for testing data\"\"\"    \n",
    "        # Check for equality amongst our custom Dataset and ImageFolder Dataset\n",
    "        # print((len(self.train_data) == len(data.train_data)) & (len(self.test_data) == len(data.test_data)))\n",
    "        print(self.train_data.classes)\n",
    "        # print(self.train_data.class_to_idx == data.train_data.class_to_idx)\n",
    "\n",
    "        self.IntoDataLoaders()\n",
    "\n",
    "    def IntoDataLoaders(self):\n",
    "        BATCH_SIZE=32\n",
    "        # NUM_WORKERS = os.cpu_count()\n",
    "        NUM_WORKERS = 1\n",
    "\n",
    "        # print(f\"number of workers avalible {NUM_WORKERS}\")\n",
    "        self.train_dataloader= DataLoader(dataset=self.train_data, # use custom created train Dataset\n",
    "                                     batch_size=BATCH_SIZE, # how many samples per batch?\n",
    "                                    #  num_workers=NUM_WORKERS, # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?\n",
    "\n",
    "        self.test_dataloader = DataLoader(dataset=self.test_data, # use custom created test Dataset\n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    # num_workers=NUM_WORKERS, \n",
    "                                    shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "        img,label=next(iter(self.test_dataloader))\n",
    "        print(f\"shape of custome dataloader img {img.shape} labels {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,in_channels,num_classes) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels,64,kernel_size=7,stride=2,padding=3)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc=nn.Linear(512,num_classes)\n",
    "\n",
    "        self.in_channels=64\n",
    "        self.out_channels=64\n",
    "\n",
    "        # self.conv2=[]\n",
    "        # for i in range(layers[0]):\n",
    "        #     self.conv2.append(nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=2,stride=1))\n",
    "        # self.bn2=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        self.conv2_1=nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.bn2_1=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        self.out_channels=self.out_channels*2\n",
    "        self.conv3_1=nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=1,stride=2)\n",
    "        self.bn3_1=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        self.in_channels=self.out_channels\n",
    "        self.out_channels=self.out_channels*2\n",
    "        self.conv4_1=nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=1,stride=2)\n",
    "        self.bn4_1=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "\n",
    "        self.in_channels=self.out_channels\n",
    "        self.out_channels=self.out_channels*2\n",
    "        self.conv5_1=nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=1,stride=2)\n",
    "        self.bn5_1=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "       \n",
    "\n",
    "        x=self.conv1(x)\n",
    "        print(f\"after conv1 channels: 64 kernel_size: 7 stride: 2 padding: 3 {x.shape}\")\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        print(f\"after maxpool kernel_size: 3 stride: 2 padding: 1 {x.shape}\")\n",
    "        identity=x\n",
    "        x=self.conv2_1(x)\n",
    "        x=self.bn2_1(x)\n",
    "        x=self.relu(x)\n",
    "        x+=identity\n",
    "        print(f\"identity shape {identity.shape}\")\n",
    "        print(f\"after conv2 channels: 64 kernel_size: 3 stride: 1 padding: 1 {x.shape}\")\n",
    "\n",
    "\n",
    "        identity=x\n",
    "        x=self.conv3_1(x)\n",
    "        x=self.bn3_1(x)\n",
    "        x=self.relu(x)\n",
    "        identity_downsample=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=1,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(num_features=128)\n",
    "        )\n",
    "        x+=identity_downsample(identity)\n",
    "        print(f\"identity shape {identity.shape}\")\n",
    "        print(f\"after conv3 channels: 128 kernel_size: 3 stride: 2 padding: 1 {x.shape}\")\n",
    "\n",
    "        \n",
    "        \n",
    "        identity=x\n",
    "        x=self.conv4_1(x)\n",
    "        x=self.bn4_1(x)\n",
    "        x=self.relu(x)\n",
    "        identity_downsample=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=1,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(num_features=256)\n",
    "        )\n",
    "        x+=identity_downsample(identity)\n",
    "        print(f\"identity shape {identity.shape}\")\n",
    "        print(f\"after conv4 channels: 256 kernel_size: 3 stride: 2 padding: 1 {x.shape}\")\n",
    "        \n",
    "\n",
    "        identity=x\n",
    "        x=self.conv5_1(x)\n",
    "        x=self.bn5_1(x)\n",
    "        x=self.relu(x)\n",
    "        identity_downsample=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=1,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(num_features=512)\n",
    "        )\n",
    "        x+=identity_downsample(identity)\n",
    "        print(f\"identity shape {identity.shape}\")\n",
    "        print(f\"after conv4 channels: 256 kernel_size: 3 stride: 2 padding: 1 {x.shape}\")\n",
    "        \n",
    "        x=self.avgpool(x)\n",
    "        print(f\"after avgpool {x.shape}\")\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        print(f\"after reshape {x.shape}\")\n",
    "\n",
    "        x=self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    # def __make_layers(self,num_residual_block,out_channels,stride):\n",
    "        \n",
    "    #     layers=[]\n",
    "    #     for i in range(num_residual_block):\n",
    "    #         layers.append(nn.Conv2d(self.in_channels,out_channels,kernel_size=3,padding=2,stride=stride))\n",
    "        \n",
    "    #     return(nn.Sequential(*layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,optimizer,loss_fn,data:CustomDataTest):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss=0\n",
    "\n",
    "    for batch, (X,y) in enumerate(data.train_dataloader):\n",
    "        y_logits=model(X)\n",
    "\n",
    "        loss=loss_fn(y_logits,y)\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss=train_loss/len(data.train_dataloader)\n",
    "    return train_loss\n",
    "\n",
    "def test_step(model,loss_fn,data:CustomDataTest):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X,y) in enumerate(data.test_dataloader):\n",
    "            y_logits=model(X)\n",
    "            loss=loss_fn(y_logits,y)\n",
    "            test_loss+=loss.item()\n",
    "\n",
    "    test_loss=test_loss/len(data.test_dataloader)\n",
    "    return test_loss\n",
    "\n",
    "def Totrain(model,data:CustomDataTest,optimizer,loss_fn,epochs:int):\n",
    "    results={\"train_loss\":[],\n",
    "             \"test_loss\":[]}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss=train_step(model,optimizer,loss_fn,data)\n",
    "        test_loss=test_step(model,loss_fn,data)\n",
    "\n",
    "                \n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pizza', 'steak', 'sushi'] {'pizza': 0, 'steak': 1, 'sushi': 2}\n",
      "['pizza', 'steak', 'sushi'] {'pizza': 0, 'steak': 1, 'sushi': 2}\n",
      "['pizza', 'steak', 'sushi']\n",
      "shape of custome dataloader img torch.Size([32, 3, 224, 224]) labels torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after conv1 channels: 64 kernel_size: 7 stride: 2 padding: 3 torch.Size([32, 64, 112, 112])\n",
      "after maxpool kernel_size: 3 stride: 2 padding: 1 torch.Size([32, 64, 56, 56])\n",
      "identity shape torch.Size([32, 64, 56, 56])\n",
      "after conv2 channels: 64 kernel_size: 3 stride: 1 padding: 1 torch.Size([32, 64, 56, 56])\n",
      "identity shape torch.Size([32, 64, 56, 56])\n",
      "after conv3 channels: 128 kernel_size: 3 stride: 2 padding: 1 torch.Size([32, 128, 28, 28])\n",
      "identity shape torch.Size([32, 128, 28, 28])\n",
      "after conv4 channels: 256 kernel_size: 3 stride: 2 padding: 1 torch.Size([32, 256, 14, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity shape torch.Size([32, 256, 14, 14])\n",
      "after conv4 channels: 256 kernel_size: 3 stride: 2 padding: 1 torch.Size([32, 512, 7, 7])\n",
      "after avgpool torch.Size([32, 512, 1, 1])\n",
      "after reshape torch.Size([32, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 512, 7, 7]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m net_optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      5\u001b[0m net_loss_fn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 6\u001b[0m res\u001b[38;5;241m=\u001b[39m\u001b[43mTotrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnet_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnet_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# x=torch.randn(32,3,224,224)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# print(f\"out shape {y.shape}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[53], line 40\u001b[0m, in \u001b[0;36mTotrain\u001b[1;34m(model, data, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     36\u001b[0m results\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[],\n\u001b[0;32m     37\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]}\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m---> 40\u001b[0m     train_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     test_loss\u001b[38;5;241m=\u001b[39mtest_step(model,loss_fn,data)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[53], line 15\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, optimizer, loss_fn, data)\u001b[0m\n\u001b[0;32m     10\u001b[0m     train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m train_loss\u001b[38;5;241m=\u001b[39mtrain_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mtrain_dataloader)\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 512, 7, 7]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "data=CustomDataTest()\n",
    "\n",
    "net=ResNet(3,len(data.train_data.classes))\n",
    "net_optimizer=torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "net_loss_fn=nn.CrossEntropyLoss()\n",
    "res=Totrain(net,data,net_optimizer,net_loss_fn,2)\n",
    "\n",
    "\n",
    "# x=torch.randn(32,3,224,224)\n",
    "# print(f\"out shape {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 512, 7, 7]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNetblock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride) -> None:\n",
    "        super(SimpleResNetblock,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=stride)\n",
    "        self.bn1=nn.BatchNorm2d(out_channels)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self,block:SimpleResNetblock,in_channels,num_classes) -> None:\n",
    "        super(ResNet18,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels,64,kernel_size=7,stride=2,padding=3)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        self.conv2=block(in_channels=64,out_channels=128,stride=1)\n",
    "\n",
    "        \n",
    "\n",
    "        self.avg=nn.AvgPool2d((1,1))\n",
    "\n",
    "        self.fc=nn.Linear(128*56*56,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "\n",
    "        x=self.avg(x)\n",
    "        # print(f\"shape after avg {x.shape}\")\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        # print(f\"shape after reshape {x.shape}\")\n",
    "        x=self.fc(x)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in shape torch.Size([2, 3, 224, 224])\n",
      "out shape torch.Size([2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:38,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 167.4154 | test_loss: 501.8230 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:08<00:33,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.9553 | test_loss: 15.5252 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:12<00:28,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 1.1484 | test_loss: 1.3048 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:16<00:24,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 1.0979 | test_loss: 1.5412 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:20<00:20,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 1.5807 | test_loss: 2.7500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:23<00:23,  4.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m net_optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      9\u001b[0m net_loss_fn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 10\u001b[0m res\u001b[38;5;241m=\u001b[39m\u001b[43mTotrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnet_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnet_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[76], line 40\u001b[0m, in \u001b[0;36mTotrain\u001b[1;34m(model, data, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     36\u001b[0m results\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[],\n\u001b[0;32m     37\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]}\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m---> 40\u001b[0m     train_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     test_loss\u001b[38;5;241m=\u001b[39mtest_step(model,loss_fn,data)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[76], line 15\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, optimizer, loss_fn, data)\u001b[0m\n\u001b[0;32m     10\u001b[0m     train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m train_loss\u001b[38;5;241m=\u001b[39mtrain_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mtrain_dataloader)\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net=ResNet18(SimpleResNetblock,3,3)\n",
    "x=torch.randn(2,3,224,224)\n",
    "print(f\"in shape {x.shape}\")\n",
    "\n",
    "y=net(x)\n",
    "print(f\"out shape {y.shape}\")\n",
    "\n",
    "net_optimizer=torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "net_loss_fn=nn.CrossEntropyLoss()\n",
    "res=Totrain(net,data,net_optimizer,net_loss_fn,3)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
