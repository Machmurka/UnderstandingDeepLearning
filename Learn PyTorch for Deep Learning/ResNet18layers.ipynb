{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1d3faebbfd0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from typing import Tuple, Dict, List\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "# https://medium.com/@karuneshu21/resnet-paper-walkthrough-b7f3bdba55f0\n",
    "# https://arxiv.org/pdf/1512.03385.pdf\n",
    "# https://medium.com/@karuneshu21/how-to-resnet-in-pytorch-9acb01f36cf5\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderCustom(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        # print(self.paths)\n",
    "        self.classes, self.class_to_idx = self.find_classes(targ_dir)\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path) \n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        img = self.load_image(index)\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return img, class_idx # return data, label (X, y)\n",
    "        \n",
    "    def find_classes(self,directory:str)->Tuple[list[str],dict[str,int]]:\n",
    "        classes=sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "        class_to_idx={cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        \n",
    "        print(classes,class_to_idx)\n",
    "        return classes,class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataTest():\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.train_dir = \"C:\\\\Users\\\\Jakub Machura\\\\source\\\\repos\\\\UnderstandingDeepLearning\\\\data\\\\pizza_steak_sushi\\\\train\"\n",
    "        self.test_dir = \"C:\\\\Users\\\\Jakub Machura\\\\source\\\\repos\\\\UnderstandingDeepLearning\\\\data\\\\pizza_steak_sushi\\\\test\"\n",
    "        \"\"\"\n",
    "            temp placement for of transform\n",
    "        \"\"\"\n",
    "        self.train_transform=transforms.Compose([\n",
    "            transforms.Resize(size=(224,224)),\n",
    "            # transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "        self.test_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "        self.train_data =ImageFolderCustom(self.train_dir,transform=self.train_transform)\n",
    "        self.test_data=ImageFolderCustom(self.test_dir,transform=self.test_transforms)\n",
    "    \n",
    "        \"\"\"temp place for testing data\"\"\"    \n",
    "        # Check for equality amongst our custom Dataset and ImageFolder Dataset\n",
    "        # print((len(self.train_data) == len(data.train_data)) & (len(self.test_data) == len(data.test_data)))\n",
    "        print(self.train_data.classes)\n",
    "        # print(self.train_data.class_to_idx == data.train_data.class_to_idx)\n",
    "\n",
    "        self.IntoDataLoaders()\n",
    "\n",
    "    def IntoDataLoaders(self):\n",
    "        BATCH_SIZE=32\n",
    "        # NUM_WORKERS = os.cpu_count()\n",
    "        NUM_WORKERS = 1\n",
    "\n",
    "        # print(f\"number of workers avalible {NUM_WORKERS}\")\n",
    "        self.train_dataloader= DataLoader(dataset=self.train_data, # use custom created train Dataset\n",
    "                                     batch_size=BATCH_SIZE, # how many samples per batch?\n",
    "                                    #  num_workers=NUM_WORKERS, # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?\n",
    "\n",
    "        self.test_dataloader = DataLoader(dataset=self.test_data, # use custom created test Dataset\n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    # num_workers=NUM_WORKERS, \n",
    "                                    shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "        img,label=next(iter(self.test_dataloader))\n",
    "        print(f\"shape of custome dataloader img {img.shape} labels {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self) -> None:\n",
    "        from torchvision.transforms import ToTensor\n",
    "        import numpy as np\n",
    "        from torch.utils.data import Subset\n",
    "        self.train_data_full =datasets.Food101(\n",
    "            root='D:\\PytorchData\\data',\n",
    "            split=\"train\",\n",
    "            download=False,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        indices = list(range(len(self.train_data_full)))\n",
    "        np.random.shuffle(indices)\n",
    "        split_idx = int(len(indices) * 0.2)\n",
    "        self.train_data = Subset(self.train_data_full, indices[:split_idx])\n",
    "\n",
    "        self.test_dat=datasets.Food101(\n",
    "            root='D:\\PytorchData\\data',\n",
    "            split=\"test\",\n",
    "            download=False,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.classes=self.train_data_full.classes\n",
    "        self.ToDataloader()\n",
    "    def ToDataloader(self):\n",
    "        BATCH_SIZE=32\n",
    "        self.train_dataloader=DataLoader(self.train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "        self.test_dataloader=DataLoader(self.test_dat,batch_size=BATCH_SIZE,shuffle=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,in_channels,num_classes) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels,64,kernel_size=7,stride=2,padding=3)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc=nn.Linear(512,num_classes)\n",
    "\n",
    "        self.in_channels=64\n",
    "        self.out_channels=64\n",
    "\n",
    "        # self.conv2=[]\n",
    "        # for i in range(layers[0]):\n",
    "        #     self.conv2.append(nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=2,stride=1))\n",
    "        # self.bn2=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        self.conv2_1=nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.bn2_1=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        self.out_channels=self.out_channels*2\n",
    "        self.conv3_1=nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=1,stride=2)\n",
    "        self.bn3_1=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        self.in_channels=self.out_channels\n",
    "        self.out_channels=self.out_channels*2\n",
    "        self.conv4_1=nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=1,stride=2)\n",
    "        self.bn4_1=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "\n",
    "        self.in_channels=self.out_channels\n",
    "        self.out_channels=self.out_channels*2\n",
    "        self.conv5_1=nn.Conv2d(self.in_channels,self.out_channels,kernel_size=3,padding=1,stride=2)\n",
    "        self.bn5_1=nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "       \n",
    "\n",
    "        x=self.conv1(x)\n",
    "        print(f\"after conv1 channels: 64 kernel_size: 7 stride: 2 padding: 3 {x.shape}\")\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        print(f\"after maxpool kernel_size: 3 stride: 2 padding: 1 {x.shape}\")\n",
    "        identity=x\n",
    "        x=self.conv2_1(x)\n",
    "        x=self.bn2_1(x)\n",
    "        x=self.relu(x)\n",
    "        x+=identity\n",
    "        print(f\"identity shape {identity.shape}\")\n",
    "        print(f\"after conv2 channels: 64 kernel_size: 3 stride: 1 padding: 1 {x.shape}\")\n",
    "\n",
    "\n",
    "        identity=x\n",
    "        x=self.conv3_1(x)\n",
    "        x=self.bn3_1(x)\n",
    "        x=self.relu(x)\n",
    "        identity_downsample=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=1,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(num_features=128)\n",
    "        )\n",
    "        x+=identity_downsample(identity)\n",
    "        print(f\"identity shape {identity.shape}\")\n",
    "        print(f\"after conv3 channels: 128 kernel_size: 3 stride: 2 padding: 1 {x.shape}\")\n",
    "\n",
    "        \n",
    "        \n",
    "        identity=x\n",
    "        x=self.conv4_1(x)\n",
    "        x=self.bn4_1(x)\n",
    "        x=self.relu(x)\n",
    "        identity_downsample=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=1,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(num_features=256)\n",
    "        )\n",
    "        x+=identity_downsample(identity)\n",
    "        print(f\"identity shape {identity.shape}\")\n",
    "        print(f\"after conv4 channels: 256 kernel_size: 3 stride: 2 padding: 1 {x.shape}\")\n",
    "        \n",
    "\n",
    "        identity=x\n",
    "        x=self.conv5_1(x)\n",
    "        x=self.bn5_1(x)\n",
    "        x=self.relu(x)\n",
    "        identity_downsample=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=1,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(num_features=512)\n",
    "        )\n",
    "        x+=identity_downsample(identity)\n",
    "        print(f\"identity shape {identity.shape}\")\n",
    "        print(f\"after conv4 channels: 256 kernel_size: 3 stride: 2 padding: 1 {x.shape}\")\n",
    "        \n",
    "        x=self.avgpool(x)\n",
    "        print(f\"after avgpool {x.shape}\")\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        print(f\"after reshape {x.shape}\")\n",
    "\n",
    "        x=self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    # def __make_layers(self,num_residual_block,out_channels,stride):\n",
    "        \n",
    "    #     layers=[]\n",
    "    #     for i in range(num_residual_block):\n",
    "    #         layers.append(nn.Conv2d(self.in_channels,out_channels,kernel_size=3,padding=2,stride=stride))\n",
    "        \n",
    "    #     return(nn.Sequential(*layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,optimizer,loss_fn,data:CustomDataTest):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss,train_acc=0,0\n",
    "\n",
    "    for batch, (X,y) in enumerate(data.train_dataloader):\n",
    "        y_logits=model(X)\n",
    "\n",
    "        loss=loss_fn(y_logits,y)\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred_class=y_logits.argmax(dim=1)\n",
    "        train_acc+=(y_pred_class==y).sum().item()/len(y_logits)\n",
    "\n",
    "    train_loss=train_loss/len(data.train_dataloader)\n",
    "    train_acc=train_acc/len(data.train_dataloader)\n",
    "    return train_loss,train_acc\n",
    "\n",
    "def test_step(model,loss_fn,data:CustomDataTest):\n",
    "    model.eval()\n",
    "    test_loss,test_acc=0,0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X,y) in enumerate(data.test_dataloader):\n",
    "            y_logits=model(X)\n",
    "            \n",
    "            loss=loss_fn(y_logits,y)\n",
    "            test_loss+=loss.item()\n",
    "\n",
    "            y_pred_class=y_logits.argmax(dim=1)\n",
    "            test_acc+=(y_pred_class==y).sum().item()/len(y_logits)\n",
    "\n",
    "    test_loss=test_loss/len(data.test_dataloader)\n",
    "    test_acc=test_acc/len(data.test_dataloader)\n",
    "\n",
    "    return test_loss,test_acc\n",
    "\n",
    "def Totrain(model,data:CustomDataTest,optimizer,loss_fn,epochs:int):\n",
    "    results={\"train_loss\":[],\n",
    "             \"train_acc\" :[],\n",
    "             \"test_loss\":[],\n",
    "             \"test_acc\": []\n",
    "             }\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss,train_acc=train_step(model,optimizer,loss_fn,data)\n",
    "        test_loss,test_acc=test_step(model,loss_fn,data)\n",
    "\n",
    "                \n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(train_acc)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot \n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=CustomDataTest()\n",
    "\n",
    "# net=ResNet(3,len(data.train_data.classes))\n",
    "# net_optimizer=torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "# net_loss_fn=nn.CrossEntropyLoss()\n",
    "# res=Totrain(net,data,net_optimizer,net_loss_fn,2)\n",
    "\n",
    "\n",
    "# x=torch.randn(32,3,224,224)\n",
    "# print(f\"out shape {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 512, 7, 7]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNetblock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride) -> None:\n",
    "        super(SimpleResNetblock,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=stride)\n",
    "        self.bn1=nn.BatchNorm2d(out_channels)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self,block:SimpleResNetblock,in_channels,num_classes) -> None:\n",
    "        super(ResNet18,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels,64,kernel_size=7,stride=2,padding=3)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        self.conv2=block(in_channels=64,out_channels=128,stride=1)\n",
    "\n",
    "        \n",
    "\n",
    "        self.avg=nn.AvgPool2d((1,1))\n",
    "\n",
    "        self.fc=nn.Linear(128*56*56,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "\n",
    "        x=self.avg(x)\n",
    "        # print(f\"shape after avg {x.shape}\")\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        # print(f\"shape after reshape {x.shape}\")\n",
    "        x=self.fc(x)\n",
    "        # print(f\"output shape {x.shape}\")\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataloader shape torch.Size([32, 3, 224, 224]) \n",
      "train dataloader shape tensor([  2,  31,  58,  26,  13,  85,  30,  60,  27,  10,  55,  25,  75,  99,\n",
      "         26,  40,  12,  24,  71,  45,  40,   1,  16,  23,  82,  75,  21, 100,\n",
      "         10,   5,   6,  15]) and len 32\n",
      "in shape torch.Size([2, 3, 224, 224])\n",
      "out shape torch.Size([2, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "data=Data()\n",
    "train_features_batch, train_labels_batch = next(iter(data.train_dataloader))\n",
    "print(f\"train dataloader shape {train_features_batch.shape} \")\n",
    "print(f\"train dataloader shape {train_labels_batch} and len {len(train_labels_batch)}\")\n",
    "\n",
    "\n",
    "net=ResNet18(SimpleResNetblock,3,len(data.classes))\n",
    "x=torch.randn(2,3,224,224)\n",
    "print(f\"in shape {x.shape}\")\n",
    "\n",
    "y=net(x)\n",
    "print(f\"out shape {y.shape}\")\n",
    "\n",
    "\n",
    "net_optimizer=torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "net_loss_fn=nn.CrossEntropyLoss()\n",
    "res=Totrain(net,data,net_optimizer,net_loss_fn,1)\n",
    "\n",
    "plot_loss_curves(res)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
