{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x16e7e764150>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from typing import Tuple, Dict, List\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "# https://medium.com/@karuneshu21/resnet-paper-walkthrough-b7f3bdba55f0\n",
    "# https://arxiv.org/pdf/1512.03385.pdf\n",
    "# https://medium.com/@karuneshu21/how-to-resnet-in-pytorch-9acb01f36cf5\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self) -> None:\n",
    "        from torchvision.transforms import ToTensor\n",
    "        import numpy as np\n",
    "        from torch.utils.data import Subset\n",
    "        self.train_data_full =datasets.Food101(\n",
    "            root='D:\\PytorchData\\data',\n",
    "            split=\"train\",\n",
    "            download=False,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        indices = list(range(len(self.train_data_full)))\n",
    "        np.random.shuffle(indices)\n",
    "        split_idx = int(len(indices) * 0.002)\n",
    "        self.train_data = Subset(self.train_data_full, indices[:split_idx])\n",
    "\n",
    "        self.test_data_full=datasets.Food101(\n",
    "            root='D:\\PytorchData\\data',\n",
    "            split=\"test\",\n",
    "            download=False,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        indices = list(range(len(self.test_data_full)))\n",
    "        np.random.shuffle(indices)\n",
    "        split_idx = int(len(indices) * 0.002)\n",
    "        self.test_data = Subset(self.test_data_full, indices[:split_idx])\n",
    "\n",
    "\n",
    "        self.classes=self.train_data_full.classes\n",
    "        self.ToDataloader()\n",
    "    def ToDataloader(self):\n",
    "        BATCH_SIZE=32\n",
    "        self.train_dataloader=DataLoader(self.train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "        self.test_dataloader=DataLoader(self.test_data,batch_size=BATCH_SIZE,shuffle=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,optimizer,loss_fn,data:Data):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss,train_acc=0,0\n",
    "\n",
    "    for batch, (X,y) in enumerate(data.train_dataloader):\n",
    "        y_logits=model(X)\n",
    "\n",
    "        loss=loss_fn(y_logits,y)\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred_class=y_logits.argmax(dim=1)\n",
    "        train_acc+=(y_pred_class==y).sum().item()/len(y_logits)\n",
    "\n",
    "    train_loss=train_loss/len(data.train_dataloader)\n",
    "    train_acc=train_acc/len(data.train_dataloader)\n",
    "    return train_loss,train_acc\n",
    "\n",
    "def test_step(model,loss_fn,data:Data):\n",
    "    model.eval()\n",
    "    test_loss,test_acc=0,0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X,y) in enumerate(data.test_dataloader):\n",
    "            y_logits=model(X)\n",
    "            \n",
    "            loss=loss_fn(y_logits,y)\n",
    "            test_loss+=loss.item()\n",
    "\n",
    "            y_pred_class=y_logits.argmax(dim=1)\n",
    "            test_acc+=(y_pred_class==y).sum().item()/len(y_logits)\n",
    "\n",
    "    test_loss=test_loss/len(data.test_dataloader)\n",
    "    test_acc=test_acc/len(data.test_dataloader)\n",
    "\n",
    "    return test_loss,test_acc\n",
    "\n",
    "def Totrain(model,data:Data,optimizer,loss_fn,epochs:int):\n",
    "    results={\"train_loss\":[],\n",
    "             \"train_acc\" :[],\n",
    "             \"test_loss\":[],\n",
    "             \"test_acc\": []\n",
    "             }\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss,train_acc=train_step(model,optimizer,loss_fn,data)\n",
    "        test_loss,test_acc=test_step(model,loss_fn,data)\n",
    "\n",
    "                \n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot \n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.ylim([0,20])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=CustomDataTest()\n",
    "\n",
    "# net=ResNet(3,len(data.train_data.classes))\n",
    "# net_optimizer=torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "# net_loss_fn=nn.CrossEntropyLoss()\n",
    "# res=Totrain(net,data,net_optimizer,net_loss_fn,2)\n",
    "\n",
    "\n",
    "# x=torch.randn(32,3,224,224)\n",
    "# print(f\"out shape {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 512, 7, 7]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNetblock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride=1,identity_downsample=None) -> None:\n",
    "        super(SimpleResNetblock,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=stride)\n",
    "        self.bn1=nn.BatchNorm2d(out_channels)\n",
    "       \n",
    "        self.conv2=nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.bn2=nn.BatchNorm2d(out_channels)\n",
    "       \n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "        self.identity_downsample=identity_downsample\n",
    "    \n",
    "    def forward(self,x):\n",
    "        identity=x\n",
    "        # print(f\"identity shape x{identity.shape}\")\n",
    "        \n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        x=self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample != None:\n",
    "            identity=self.identity_downsample(identity)\n",
    "            # print(f\"identity shape after {identity.shape}\")\n",
    "            # print(f\"x shape {x.shape}\")\n",
    "\n",
    "        x+=identity\n",
    "        x=self.relu(x)\n",
    "        # print(f\"x shape {x.shape}\")\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self,block:SimpleResNetblock,img_channels,num_classes,block_num:list) -> None:\n",
    "        super(ResNet18,self).__init__()\n",
    "        \n",
    "        self.in_channels=64\n",
    "\n",
    "        self.conv1=nn.Conv2d(img_channels,64,kernel_size=7,stride=2,padding=3)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        self.layer2=self._make_layer(block,block_num[0],64,1)\n",
    "        self.layer3=self._make_layer(block,block_num[1],128,2)\n",
    "        self.layer4=self._make_layer(block,block_num[2],256,2)\n",
    "        self.layer5=self._make_layer(block,block_num[3],512,2)\n",
    "\n",
    "        \n",
    "\n",
    "        self.avg=nn.AvgPool2d((1,1))\n",
    "        self.fc=nn.Linear(512*7*7,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        x=self.layer5(x)\n",
    "\n",
    "        \n",
    "        x=self.avg(x)\n",
    "        # print(f\"shape after avg {x.shape}\")\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        # print(f\"shape after reshape {x.shape}\")\n",
    "        x=self.fc(x)\n",
    "        # print(f\"output shape {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _make_layer(self,block,num_blocks,out_channels,stride):\n",
    "        identity_downsample=None\n",
    "        layers=[]\n",
    "        if stride!=1:\n",
    "            identity_downsample=nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels,out_channels,1,stride,padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        layers.append(block(self.in_channels,out_channels=out_channels,stride=stride,identity_downsample=identity_downsample))\n",
    "        self.in_channels=out_channels\n",
    "\n",
    "        for i in range(num_blocks-1):\n",
    "            layers.append(block(self.in_channels,out_channels=out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_dataloader 160\n",
      "len test_dataloader 64\n",
      "in shape torch.Size([2, 3, 224, 224])\n",
      "out shape torch.Size([2, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m net_optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     17\u001b[0m net_loss_fn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 18\u001b[0m res\u001b[38;5;241m=\u001b[39m\u001b[43mTotrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnet_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnet_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m plot_loss_curves(res)\n",
      "Cell \u001b[1;32mIn[68], line 53\u001b[0m, in \u001b[0;36mTotrain\u001b[1;34m(model, data, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     46\u001b[0m results\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[],\n\u001b[0;32m     47\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m :[],\n\u001b[0;32m     48\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[],\n\u001b[0;32m     49\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m     50\u001b[0m          }\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m---> 53\u001b[0m     train_loss,train_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     test_loss,test_acc\u001b[38;5;241m=\u001b[39mtest_step(model,loss_fn,data)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[68], line 6\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, optimizer, loss_fn, data)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m train_loss,train_acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\food101.py:74\u001b[0m, in \u001b[0;36mFood101.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m     73\u001b[0m     image_file, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image_files[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels[idx]\n\u001b[1;32m---> 74\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     77\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3231\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3229\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fp, Path):\n\u001b[1;32m-> 3231\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   3232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_path(fp):\n\u001b[0;32m   3233\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n",
      "File \u001b[1;32mc:\\Users\\Jakub Machura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py:993\u001b[0m, in \u001b[0;36mPath.resolve\u001b[1;34m(self, strict)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymlink loop from \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\u001b[38;5;241m.\u001b[39mfilename)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 993\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    995\u001b[0m     check_eloop(e)\n",
      "File \u001b[1;32m<frozen ntpath>:696\u001b[0m, in \u001b[0;36mrealpath\u001b[1;34m(path, strict)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data=Data()\n",
    "train_features_batch, train_labels_batch = next(iter(data.train_dataloader))\n",
    "print(f\"len train_dataloader {len(data.train_dataloader)*32}\")\n",
    "print(f\"len test_dataloader {len(data.test_dataloader)*32}\")\n",
    "\n",
    "\n",
    "\n",
    "net=ResNet18(SimpleResNetblock,3,len(data.classes),[3,4,6,3])\n",
    "x=torch.randn(2,3,224,224)\n",
    "print(f\"in shape {x.shape}\")\n",
    "\n",
    "y=net(x)\n",
    "print(f\"out shape {y.shape}\")\n",
    "\n",
    "\n",
    "net_optimizer=torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "net_loss_fn=nn.CrossEntropyLoss()\n",
    "res=Totrain(net,data,net_optimizer,net_loss_fn,4)\n",
    "\n",
    "plot_loss_curves(res)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetblock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride=1,identity_downsample=None) -> None:\n",
    "        super(ResNetblock,self).__init__()\n",
    "\n",
    "        self.expansion=4\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=stride)\n",
    "        self.bn1=nn.BatchNorm2d(out_channels)\n",
    "       \n",
    "        self.conv2=nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.bn2=nn.BatchNorm2d(out_channels)\n",
    "       \n",
    "        self.conv3=nn.Conv2d(in_channels=out_channels,out_channels=out_channels*self.expansion,kernel_size=3,padding=1,stride=1)\n",
    "        self.bn3=nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        \n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "        self.identity_downsample=identity_downsample\n",
    "    \n",
    "    def forward(self,x):\n",
    "        identity=x\n",
    "        # print(f\"identity shape x{identity.shape}\")\n",
    "        \n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        # print(f\"after conv1 shape {x.shape}\")\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        x=self.bn2(x)\n",
    "        # print(f\"after conv2 shape {x.shape}\")\n",
    "\n",
    "        # print(f\"before conv3 shape {x.shape}\")\n",
    "        x=self.conv3(x)\n",
    "        x=self.bn3(x)\n",
    "        # print(f\"after conv3 shape {x.shape}\")\n",
    "        \n",
    "        if self.identity_downsample != None:\n",
    "            identity=self.identity_downsample(identity)\n",
    "            # print(f\"identity shape after downsample {identity.shape}\")\n",
    "\n",
    "        x+=identity\n",
    "        x=self.relu(x)\n",
    "        # print(f\"x shape {x.shape}\")\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self,block:ResNetblock,img_channels,num_classes,block_num:list) -> None:\n",
    "        super(ResNet50,self).__init__()\n",
    "        \n",
    "        self.in_channels=64\n",
    "\n",
    "        self.conv1=nn.Conv2d(img_channels,64,kernel_size=7,stride=2,padding=3)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        self.layer2=self._make_layer(block,block_num[0],64,1)\n",
    "        self.layer3=self._make_layer(block,block_num[1],128,2)\n",
    "        self.layer4=self._make_layer(block,block_num[2],256,2)\n",
    "        self.layer5=self._make_layer(block,block_num[3],512,2)\n",
    "\n",
    "        self.avg=nn.AvgPool2d((1,1))\n",
    "        self.fc=nn.Linear(2048*7*7,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        \n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        x=self.layer5(x)\n",
    "\n",
    "\n",
    "        x=self.avg(x)\n",
    "        # print(f\"shape after avg {x.shape}\")\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        # print(f\"shape after reshape {x.shape}\")\n",
    "        x=self.fc(x)\n",
    "        # print(f\"output shape {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _make_layer(self,block:ResNetblock,num_blocks,out_channels,stride):\n",
    "        identity_downsample=None\n",
    "        layers=[]\n",
    "\n",
    "        if stride!=1 or self.in_channels!=out_channels*4:\n",
    "            identity_downsample=nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels,out_channels*4,1,stride,padding=0),\n",
    "                nn.BatchNorm2d(out_channels*4)\n",
    "            )\n",
    "            # print(\"stride test\")\n",
    "        \n",
    "        layers.append(block(self.in_channels,out_channels=out_channels,stride=stride,identity_downsample=identity_downsample))\n",
    "        self.in_channels=out_channels*4\n",
    "\n",
    "        for i in range(num_blocks-1):\n",
    "            layers.append(block(self.in_channels,out_channels=out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after maxpool shape torch.Size([2, 64, 56, 56])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(2,3,224,224)\n",
    "\n",
    "net50=ResNet50(ResNetblock,3,4,[3,4,6,3])\n",
    "\n",
    "y=net50(x)\n",
    "\n",
    "print(y.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
