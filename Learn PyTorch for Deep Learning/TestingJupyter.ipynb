{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "from typing import Tuple, Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer \n",
    "\n",
    "\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderCustom(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        print(self.pat)\n",
    "        self.classes, self.class_to_idx = self.find_classes(targ_dir)\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path) \n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        img = self.load_image(index)\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return img, class_idx # return data, label (X, y)\n",
    "        \n",
    "    def find_classes(self,directory:str)->Tuple[list[str],dict[str,int]]:\n",
    "        classes=sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "        class_to_idx={cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        \n",
    "        print(classes,class_to_idx)\n",
    "        return classes,class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataTest():\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.train_dir = \"data\\\\pizza_steak_sushi\\\\train\"\n",
    "        self.test_dir = \"data\\\\pizza_steak_sushi\\\\test\"\n",
    "        \"\"\"\n",
    "            temp placement for of transform\n",
    "        \"\"\"\n",
    "        self.train_transform=transforms.Compose([\n",
    "            transforms.Resize(size=(64,64)),\n",
    "            transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "        self.test_transforms = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "        self.train_data_custom =ImageFolderCustom(self.train_dir,transform=self.train_transform)\n",
    "        self.test_data_custom=ImageFolderCustom(self.test_dir,transform=self.test_transforms)\n",
    "    \n",
    "        \"\"\"temp place for testing data\"\"\"    \n",
    "        # Check for equality amongst our custom Dataset and ImageFolder Dataset\n",
    "        # print((len(self.train_data_custom) == len(data.train_data)) & (len(self.test_data_custom) == len(data.test_data)))\n",
    "        # print(self.train_data_custom.classes == data.train_data.classes)\n",
    "        # print(self.train_data_custom.class_to_idx == data.train_data.class_to_idx)\n",
    "\n",
    "        # self.IntoDataLoaders()\n",
    "\n",
    "    def IntoDataLoaders(self):\n",
    "        BATCH_SIZE=32\n",
    "        NUM_WORKERS = os.cpu_count()\n",
    "        print(f\"number of workers avalible {NUM_WORKERS}\")\n",
    "        self.train_dataloader= DataLoader(dataset=self.train_data_custom, # use custom created train Dataset\n",
    "                                     batch_size=BATCH_SIZE, # how many samples per batch?\n",
    "                                     num_workers=NUM_WORKERS, # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?\n",
    "\n",
    "        self.test_dataloader = DataLoader(dataset=self.test_data_custom, # use custom created test Dataset\n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    num_workers=NUM_WORKERS, \n",
    "                                    shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "        img,label=next(iter(self.test_dataloader))\n",
    "        print(f\"shape of custome dataloader img {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinnyVGG(nn.Module):\n",
    "    def __init__(self,in_shape:int,out_shape:int,hidden_units:int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*16*16,out_features=out_shape)\n",
    "        )\n",
    "        print(self.state_dict)\n",
    "    def forward(self,x):\n",
    "        return(self.classifier(self.conv_block_2(self.conv_block1(x))))\n",
    "    \n",
    "    def ShapeCheck(self,data:CustomDataTest):\n",
    "        img_batch,label_batch=next(iter(data.test_dataloader))\n",
    "\n",
    "        img_single,label_single=img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "        \n",
    "        print(f\"single img shape {img_single.shape}\")\n",
    "\n",
    "        self.eval()\n",
    "        with torch.inference_mode():\n",
    "            y=self(img_single)\n",
    "\n",
    "        print(f\"shape of output raw y\\n{y.shape}\")\n",
    "        print(f\"output of pred label \\n{torch.argmax(torch.softmax(y,dim=1),dim=1)}\")\n",
    "        print(f\"actual label \\n{label_single}\")\n",
    "    \n",
    "    def train_step(self,loss_fn,optimizer,data):\n",
    "        self.train()\n",
    "\n",
    "        train_loss,train_acc=0, 0\n",
    "\n",
    "        for batch, (X,y) in enumerate(data.train_dataloader):\n",
    "            y_logits=self(X)\n",
    "\n",
    "            loss=loss_fn(y_logits,y)\n",
    "            train_loss+=loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            y_pred_class = y_logits.argmax(dim=1)\n",
    "            train_acc += (y_pred_class == y).sum().item()/len(y_pred_class)\n",
    "\n",
    "        train_loss=train_loss/len(data.train_dataloader)\n",
    "        train_acc=train_loss/len(data.train_dataloader)\n",
    "        return train_loss,train_acc\n",
    "    \n",
    "    def test_step(self,loss_fn,data:CustomDataTest):\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "        test_loss,test_acc=0,0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for batch, (X,y) in enumerate(data.test_dataloader):\n",
    "                y_logits=self(X)\n",
    "\n",
    "                loss=loss_fn(y_logits,y)\n",
    "                test_loss+=loss.item()\n",
    "\n",
    "                test_pred_labels = y_logits.argmax(dim=1)\n",
    "                test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "\n",
    "        test_loss=test_loss/len(data.test_dataloader)\n",
    "        test_acc=test_acc/len(data.test_dataloader)\n",
    "        return test_loss, test_acc\n",
    "\n",
    "    def Totrain(self,data:CustomDataTest,epochs:int,optimizer,loss_fn:torch.nn.Module):\n",
    "        \n",
    "        self.results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            train_loss,train_acc=self.train_step(loss_fn,optimizer,data)\n",
    "\n",
    "            test_loss,test_acc=self.test_step(loss_fn,data)\n",
    "\n",
    "                    # 4. Print out what's happening\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"train_loss: {train_loss:.4f} | \"\n",
    "                f\"train_acc: {train_acc:.4f} | \"\n",
    "                f\"test_loss: {test_loss:.4f} | \"\n",
    "                f\"test_acc: {test_acc:.4f}\"\n",
    "            )\n",
    "    \n",
    "            # 5. Update results dictionary\n",
    "            self.results[\"train_loss\"].append(train_loss)\n",
    "            self.results[\"train_acc\"].append(train_acc)\n",
    "            self.results[\"test_loss\"].append(test_loss)\n",
    "            self.results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot \n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir = \"data\\\\pizza_steak_sushi\\\\train\"\n",
    "# print(os.path.dirname(train_dir))\n",
    "# print(Path(train_dir).glob(\"*/*.jpg\"))\n",
    "\n",
    "# list(Path(targ_dir).glob(\"*/*.jpg\"))\n",
    "\n",
    "data1=CustomDataTest()\n",
    "# model0=TinnyVGG(3,len(data1.train_data_custom.classes),10)\n",
    "\n",
    "# model0_optimizer=torch.optim.Adam(params=model0.parameters(),lr=0.001)\n",
    "# model0_loss_fn=nn.CrossEntropyLoss()\n",
    "# start_time=timer()\n",
    "# model0_results=model0.Totrain(data=data1,epochs=5,optimizer=model0_optimizer,loss_fn=model0_loss_fn)\n",
    "\n",
    "# end_time=timer()\n",
    "# print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
    "# plot_loss_curves(model0_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
